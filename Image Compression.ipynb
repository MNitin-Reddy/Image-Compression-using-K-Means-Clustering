{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b113cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfe17e",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Implementing K-means\n",
    "\n",
    "The K-means algorithm is a method to automatically cluster similar\n",
    "data points together. \n",
    "\n",
    "* Concretely, you are given a training set $\\{x^{(1)}, ..., x^{(m)}\\}$, and you want\n",
    "to group the data into a few cohesive “clusters”. \n",
    "\n",
    "\n",
    "* K-means is an iterative procedure that\n",
    "     * Starts by guessing the initial centroids, and then \n",
    "     * Refines this guess by \n",
    "         * Repeatedly assigning examples to their closest centroids, and then \n",
    "         * Recomputing the centroids based on the assignments.\n",
    "         \n",
    "\n",
    "* In pseudocode, the K-means algorithm is as follows:\n",
    "\n",
    "    ``` python\n",
    "    # Initialize centroids\n",
    "    # K is the number of clusters\n",
    "    centroids = kMeans_init_centroids(X, K)\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        # Cluster assignment step: \n",
    "        # Assign each data point to the closest centroid. \n",
    "        # idx[i] corresponds to the index of the centroid \n",
    "        # assigned to example i\n",
    "        idx = find_closest_centroids(X, centroids)\n",
    "\n",
    "        # Move centroid step: \n",
    "        # Compute means based on centroid assignments\n",
    "        centroids = compute_centroids(X, idx, K)\n",
    "    ```\n",
    "\n",
    "* The inner-loop of the algorithm repeatedly carries out two steps: \n",
    "    1. Assigning each training example $x^{(i)}$ to its closest centroid, and\n",
    "    2. Recomputing the mean of each centroid using the points assigned to it. \n",
    "    \n",
    "    \n",
    "* The $K$-means algorithm will always converge to some final set of means for the centroids. \n",
    "\n",
    "* However, the converged solution may not always be ideal and depends on the initial setting of the centroids.\n",
    "    * Therefore, in practice the K-means algorithm is usually run a few times with different random initializations. \n",
    "    * One way to choose between these different solutions from different random initializations is to choose the one with the lowest cost function value (distortion).\n",
    "\n",
    "We will implement the two phases of the K-means algorithm separately in the next sections. \n",
    "* We will start by completing `find_closest_centroid` and then proceed to complete `compute_centroids`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f7e644",
   "metadata": {},
   "source": [
    "<a name=\"1.1\"></a>\n",
    "### 1.1 Finding closest centroids\n",
    "\n",
    "In the “cluster assignment” phase of the K-means algorithm, the\n",
    "algorithm assigns every training example $x^{(i)}$ to its closest\n",
    "centroid, given the current positions of centroids. \n",
    "\n",
    "`find_closest_centroids`. \n",
    "* This function takes the data matrix `X` and the locations of all\n",
    "centroids inside `centroids` \n",
    "* It should output a one-dimensional array `idx` (which has the same number of elements as `X`) that holds the index  of the closest centroid (a value in $\\{0,...,K-1\\}$, where $K$ is total number of centroids) to every training example . *(Note: The index range 0 to K-1 varies slightly from what is shown in the lectures (i.e. 1 to K) because Python list indices start at 0 instead of 1)*\n",
    "* Specifically, for every example $x^{(i)}$ we set\n",
    "$$c^{(i)} := j \\quad \\mathrm{that \\; minimizes} \\quad ||x^{(i)} - \\mu_j||^2,$$\n",
    "where \n",
    " * $c^{(i)}$ is the index of the centroid that is closest to $x^{(i)}$ (corresponds to `idx[i]` in the starter code), and \n",
    " * $\\mu_j$ is the position (value) of the $j$’th centroid. (stored in `centroids` in the starter code)\n",
    " * $||x^{(i)} - \\mu_j||$ is the L2-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb3b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_centroids(X, centroids):\n",
    "    \"\"\"\n",
    "    Computes the centroid memberships for every example\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Input values      \n",
    "        centroids (ndarray): (K, n) centroids\n",
    "    \n",
    "    Returns:\n",
    "        idx (array_like): (m,) closest centroids\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Set K\n",
    "    K = centroids.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        distances = np.zeros(K)  # Store distances from the point to each centroid\n",
    "        for j in range(K):\n",
    "            # Compute the Euclidean distance between X[i] and the j-th centroid\n",
    "            distances[j] = np.linalg.norm(X[i] - centroids[j])\n",
    "        \n",
    "        # Find the index of the closest centroid\n",
    "        idx[i] = np.argmin(distances)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db478578",
   "metadata": {},
   "source": [
    "### 1.2 Computing centroid means\n",
    "\n",
    "Given assignments of every point to a centroid, the second phase of the\n",
    "algorithm recomputes, for each centroid, the mean of the points that\n",
    "were assigned to it.\n",
    "\n",
    "\n",
    "<a name=\"ex02\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69fb6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids by computing the means of the \n",
    "    data points assigned to each centroid.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):   (m, n) Data points\n",
    "        idx (ndarray): (m,) Array containing index of closest centroid for each \n",
    "                       example in X. Concretely, idx[i] contains the index of \n",
    "                       the centroid closest to example i\n",
    "        K (int):       number of centroids\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): (K, n) New centroids computed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Useful variables\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    centroids = np.zeros((K, n))\n",
    "    for k in range(K):\n",
    "        # Get all points assigned to the k-th centroid\n",
    "        points = X[idx == k]\n",
    "        \n",
    "        # Compute the mean of those points to find the new centroid position\n",
    "        if len(points) > 0:\n",
    "            centroids[k] = np.mean(points, axis=0)\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84355868",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - K-means on a sample dataset \n",
    "\n",
    "After you have completed the two functions (`find_closest_centroids`\n",
    "and `compute_centroids`) above, the next step is to run the\n",
    "K-means algorithm on a toy 2D dataset to help you understand how\n",
    "K-means works. \n",
    "* We encourage you to take a look at the function (`run_kMeans`) below to understand how it works. \n",
    "* Notice that the code calls the two functions you implemented in a loop.\n",
    "\n",
    "When you run the code below, it will produce a\n",
    "visualization that steps through the progress of the algorithm at\n",
    "each iteration. \n",
    "* At the end, your figure should look like the one displayed in Figure 1.\n",
    "* The final centroids are the black X-marks in the middle of the colored clusters.\n",
    "* You can see how these centroids got to their final location by looking at the other X-marks connected to it.\n",
    "\n",
    "<img src=\"images/figure 1.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4f0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kMeans(X, initial_centroids, max_iters=10, plot_progress=False):\n",
    "    \"\"\"\n",
    "    Runs the K-Means algorithm on data matrix X, where each row of X\n",
    "    is a single example\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize values\n",
    "    m, n = X.shape\n",
    "    K = initial_centroids.shape[0]\n",
    "    centroids = initial_centroids\n",
    "    previous_centroids = centroids    \n",
    "    idx = np.zeros(m)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Run K-Means\n",
    "    for i in range(max_iters):\n",
    "        \n",
    "        #Output progress\n",
    "        print(\"K-Means iteration %d/%d\" % (i, max_iters-1))\n",
    "        \n",
    "        # For each example in X, assign it to the closest centroid\n",
    "        idx = find_closest_centroids(X, centroids)\n",
    "        \n",
    "        # Optionally plot progress\n",
    "        if plot_progress:\n",
    "            plot_progress_kMeans(X, centroids, previous_centroids, idx, K, i)\n",
    "            previous_centroids = centroids\n",
    "            \n",
    "        # Given the memberships, compute new centroids\n",
    "        centroids = compute_centroids(X, idx, K)\n",
    "    plt.show() \n",
    "    return centroids, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39bdfb",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Random initialization\n",
    "\n",
    "The initial assignments of centroids for the example dataset was designed so that you will see the same figure as in Figure 1. In practice, a good strategy for initializing the centroids is to select random examples from the\n",
    "training set.\n",
    "\n",
    "In this part of the exercise, you should understand how the function `kMeans_init_centroids` is implemented.\n",
    "* The code first randomly shuffles the indices of the examples (using `np.random.permutation()`). \n",
    "* Then, it selects the first $K$ examples based on the random permutation of the indices. \n",
    "* This allows the examples to be selected at random without the risk of selecting the same example twice.\n",
    "\n",
    "**Note**: You do not need to implement anything for this part of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4685288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans_init_centroids(X, K):\n",
    "    \"\"\"\n",
    "    This function initializes K centroids that are to be \n",
    "    used in K-Means on the dataset X\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Data points \n",
    "        K (int):     number of centroids/clusters\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): Initialized centroids\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomly reorder the indices of examples\n",
    "    randidx = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    # Take the first K examples as centroids\n",
    "    centroids = X[randidx[:K]]\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4eb7bd",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Image compression with K-means\n",
    "\n",
    "\n",
    "* In a straightforward 24-bit color representation of an image$^{2}$, each pixel is represented as three 8-bit unsigned integers (ranging from 0 to 255) that specify the red, green and blue intensity values. This encoding is often refered to as the RGB encoding.\n",
    "* Our image contains thousands of colors, and in this part of the exercise, you will reduce the number of\n",
    "colors to 16 colors.\n",
    "* By making this reduction, it is possible to represent (compress) the photo in an efficient way. \n",
    "* Specifically, you only need to store the RGB values of the 16 selected colors, and for each pixel in the image you now need to only store the index of the color at that location (where only 4 bits are necessary to represent 16 possibilities).\n",
    "\n",
    "In this part, you will use the K-means algorithm to select the 16 colors that will be used to represent the compressed image.\n",
    "* Concretely, you will treat every pixel in the original image as a data example and use the K-means algorithm to find the 16 colors that best group (cluster) the pixels in the 3- dimensional RGB space.\n",
    "* Once you have computed the cluster centroids on the image, you will then use the 16 colors to replace the pixels in the original image.\n",
    "\n",
    "<img src=\"images/figure 2.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "$^{2}$<sub>The provided photo used in this exercise belongs to Frank Wouters and is used with his permission.</sub>\n",
    "\n",
    "<a name=\"4.1\"></a>\n",
    "### 4.1 Dataset\n",
    "\n",
    "**Load image**\n",
    "\n",
    "First, we will use `matplotlib` to read in the original image, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73f77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff74268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
